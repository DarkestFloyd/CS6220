{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy.stats import entropy as kl_div\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/DUC/'\n",
    "docs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(path):\n",
    "    import os\n",
    "    from bs4 import BeautifulSoup\n",
    "    ret_dict = {}\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        file = path + file\n",
    "        if os.path.isdir(file) or '.txt' in file:\n",
    "            continue\n",
    "        with open(path + file) as infile:\n",
    "            soup = BeautifulSoup(infile, 'html.parser')\n",
    "            ret_dict[soup.docno.text.strip()] = soup.find('text').text.strip()\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 535 ms, sys: 40.9 ms, total: 576 ms\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs = get_docs(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "duc_data = [v for k, v in docs.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KlSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r\"(\\W)\")\n",
    "wc = lambda text: Counter([t for t in re.split(r\"(\\W)\", text) if t and t != ' ' and t != '\\n'])\n",
    "pd = lambda wc: {k: v/sum(wc.values()) for k, v in wc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klsum(document, summary, L):\n",
    "    doc_sent = sent_tokenize(document)\n",
    "    doc_wc = wc(document)\n",
    "    doc_pd = pd(doc_wc)\n",
    "    px = [p for p in doc_pd.values()]\n",
    "    \n",
    "    while len(sent_tokenize(summary)) < L:\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        _min, _min_id = 999, -1\n",
    "        for idx, sent in enumerate(sentences):\n",
    "            new_sum = summary + sent\n",
    "            new_pd = pd(wc(new_sum))\n",
    "            qx = [new_pd[k] if k in new_pd else 0.001 for k in doc_pd.keys()]\n",
    "            kl = kl_div(px, qx)\n",
    "            if kl < _min:\n",
    "                _min, _min_id = kl, idx\n",
    "        \n",
    "        summary += \"\\n\" + sentences[_min_id]\n",
    "        document = \" \".join(sentences[:_min_id] + sentences[_min_id+1:])\n",
    "    \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Census officials say they are required to count everyone by the\n",
      "U.S. Constitution, which does not mention citizenship but only\n",
      "instructs that the House apportionment be based on the ``whole\n",
      "number of persons'' residing in the various states.\n",
      "\n",
      "\n",
      "Rep. Tom Ridge, R-Pa., said the Census Bureau should actually\n",
      "count everyone but that it should develop a method to determine how\n",
      "many people are illegally in the country, and them deduct that\n",
      "number from the figures used for reapportioning Congress.\n",
      "CPU times: user 7.98 ms, sys: 10 Âµs, total: 7.99 ms\n",
      "Wall time: 7.67 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(klsum_new(docs['AP880217-0175'], '', 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TopicSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=-1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=666, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2, max_features=1000)\n",
    "features = vectorizer.fit_transform(duc_data)\n",
    "model = LatentDirichletAllocation(n_components=10, random_state=666, learning_method='online', n_jobs=-1)\n",
    "model.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topicsum(document, summary, L):\n",
    "    px = model.transform(vectorizer.transform([document]))[0]\n",
    "    \n",
    "    while len(sent_tokenize(summary)) < L:\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        _min, _min_id = 999, -1\n",
    "        for idx, sent in enumerate(sentences):\n",
    "            new_sum = summary + sent\n",
    "            qx = model.transform(vectorizer.transform([new_sum]))[0]\n",
    "            kl = kl_div(px, qx)\n",
    "            if kl < _min:\n",
    "                _min, _min_id = kl, idx\n",
    "        \n",
    "        summary += \"\\n\" + sentences[_min_id]\n",
    "        document = \" \".join(sentences[:_min_id] + sentences[_min_id+1:])\n",
    "    \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The group contends that including the estimated 2 million or\n",
      "more illegal aliens in the national head count, which is used to\n",
      "distribute seats in the House of Representatives, will cause unfair\n",
      "shifts of seats from one state to another.\n",
      "Some 40 members of the House joined the Federation for American\n",
      "Immigration Reform in announcing that the suit would be filed\n",
      "Thursday in U.S. District Court in Pittsburgh, spokesmen said at a\n",
      "news conference here.\n"
     ]
    }
   ],
   "source": [
    "print(topicsum(docs['AP880217-0175'], '', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
