{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy.stats import entropy as kl_div\n",
    "from math import log\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index ES for topics and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexer(index, doc_type, body):\n",
    "    data = es.search(index=index, doc_type=doc_type, body=body)['hits']['hits']\n",
    "    data = [d['_source'] for d in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_text = {\n",
    "    'size' : 301,\n",
    "    'query': {\n",
    "        'match_all' : {}\n",
    "    }\n",
    "}\n",
    "duc_es = indexer('ducsummary', 'doc', _text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_text = {\n",
    "    'size' : 500,\n",
    "    'query': {\n",
    "        'match_all' : {}\n",
    "    }\n",
    "}\n",
    "_20ng_es = indexer('20ngsummary', 'doc', _text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_topics = {\n",
    "    'size' : 10,\n",
    "    'query': {\n",
    "        'match_all' : {}\n",
    "    }\n",
    "}\n",
    "duc_topics = indexer('topicsduc', 'topic', _topics)\n",
    "_20ng_topics = indexer('topics20ng', 'topic', _topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KlSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r\"(\\W)\")\n",
    "wc = lambda text: Counter([t for t in re.split(r\"(\\W)\", text) if t and t != ' ' and t != '\\n'])\n",
    "pd = lambda wc: {k: v/sum(wc.values()) for k, v in wc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klsum(document, summary, L):\n",
    "    doc_sent = sent_tokenize(document)\n",
    "    doc_wc = wc(document)\n",
    "    doc_pd = pd(doc_wc)\n",
    "    px = [p for p in doc_pd.values()]\n",
    "    if len(doc_sent) == 1 or len(doc_sent) == 0:\n",
    "        return document.strip()\n",
    "    \n",
    "    while len(sent_tokenize(summary)) < L:\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        _min, _min_id = 999, -1\n",
    "        for idx, sent in enumerate(sentences):\n",
    "            new_sum = summary + sent\n",
    "            new_pd = pd(wc(new_sum))\n",
    "            qx = [new_pd[k] if k in new_pd else 0.001 for k in doc_pd.keys()]\n",
    "            kl = kl_div(px, qx)\n",
    "            if kl < _min:\n",
    "                _min, _min_id = kl, idx\n",
    "        \n",
    "        summary += \"\\n\" + sentences[_min_id]\n",
    "        document = \" \".join(sentences[:_min_id] + sentences[_min_id+1:])\n",
    "    \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "duc_kl = {}\n",
    "for d in duc_es:\n",
    "    duc_kl[d['doc_id']] = klsum(d['doc_text'], '', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [00:01, 395.96it/s]\n"
     ]
    }
   ],
   "source": [
    "_20ng_kl = {}\n",
    "for i, d in tqdm(enumerate(_20ng_es)):\n",
    "    _20ng_kl[d['doc_id']] = klsum(d['doc_text'], '', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDASum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldasum(data, summary, topics, L):\n",
    "    doc = data['doc_text']\n",
    "    doc_topics = [int(t.strip()) for t in data['doc_topics'].split(',')]\n",
    "    doc_topics_pd = [float(t.strip()) for t in data['doc_topics_pd'].split(',')]\n",
    "    \n",
    "    if len(sent_tokenize(doc)) == 0 or len(sent_tokenize(doc)) == 1:\n",
    "        return doc\n",
    "    \n",
    "    while len(sent_tokenize(summary)) < L:\n",
    "        sentences = sent_tokenize(doc)\n",
    "        score = [0] * len(sentences)\n",
    "        for idx, topic in enumerate(topics):\n",
    "            if topic['topic_id'] in doc_topics:\n",
    "                for sidx, sent in enumerate(sentences):\n",
    "                    for word in word_tokenize(sent):\n",
    "                        if word in topic['top_words']:\n",
    "                            score[sidx] += topic['word_prob'][topic['top_words'].index(word)]\n",
    "        \n",
    "        best = np.argmax(score)\n",
    "        summary += ' \\n ' + sentences[best]\n",
    "        doc = \" \".join(sentences[:best] + sentences[best+1:])\n",
    "        \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 91.02it/s]\n"
     ]
    }
   ],
   "source": [
    "_20_lda = {}\n",
    "for d in tqdm(_20ng_es):\n",
    "    _20_lda[d['doc_id']] = ldasum(d, '', _20ng_topics, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 301/301 [00:13<00:00, 21.57it/s]\n"
     ]
    }
   ],
   "source": [
    "duc_lda = {}\n",
    "for d in tqdm(duc_es):\n",
    "    duc_lda[d['doc_id']] = ldasum(d, '', duc_topics, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:35<00:00, 14.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(_20ng_es):\n",
    "    _id = d['doc_id']\n",
    "    d['kl_summary'] = _20ng_kl[_id]\n",
    "    d['lda_summary'] = _20_lda[_id]\n",
    "    \n",
    "    es.index(index='20ngsummary', doc_type='doc', body=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "duc_data = [d['doc_text'] for d in duc_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=-1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=666, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2, max_features=1000)\n",
    "features = vectorizer.fit_transform(duc_data)\n",
    "model = LatentDirichletAllocation(n_components=10, random_state=666, learning_method='online', n_jobs=-1)\n",
    "model.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topicsum(document, summary, L):\n",
    "    px = model.transform(vectorizer.transform([document]))[0]\n",
    "    \n",
    "    while len(sent_tokenize(summary)) < L:\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        _min, _min_id = 999, -1\n",
    "        for idx, sent in enumerate(sentences):\n",
    "            new_sum = summary + sent\n",
    "            qx = model.transform(vectorizer.transform([new_sum]))[0]\n",
    "            kl = kl_div(px, qx)\n",
    "            if kl < _min:\n",
    "                _min, _min_id = kl, idx\n",
    "        \n",
    "        summary += \"\\n\" + sentences[_min_id]\n",
    "        document = \" \".join(sentences[:_min_id] + sentences[_min_id+1:])\n",
    "    \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in duc_es:\n",
    "    if d['doc_id'] == 'AP880217-0175':\n",
    "        test = d['doc_text']\n",
    "        ref = d['gold_summary']\n",
    "        lda = d['lda_summary']\n",
    "        kl = d['kl_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = topicsum(test, '', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333284621247"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(hyp, ref)[0]['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3053435073970049"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(lda, ref)[0]['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0312499960986333"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(kl, ref)[0]['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like adding PD and PS topic distributions do give better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
