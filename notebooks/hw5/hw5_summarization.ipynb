{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy.stats import entropy as kl_div\n",
    "from math import log\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index ES for topics and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_text = {\n",
    "    'size' : 301,\n",
    "    'query': {\n",
    "        'match_all' : {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = es.search(index='duc', doc_type='doc', body=_text)['hits']['hits']\n",
    "data = [d['_source'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "_topics = {\n",
    "    'size' : 10,\n",
    "    'query': {\n",
    "        'match_all' : {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = es.search(index='topicsduc', doc_type='topic', body=_topics)['hits']['hits']\n",
    "topics = [t['_source'] for t in topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KlSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r\"(\\W)\")\n",
    "wc = lambda text: Counter([t for t in re.split(r\"(\\W)\", text) if t and t != ' ' and t != '\\n'])\n",
    "pd = lambda wc: {k: v/sum(wc.values()) for k, v in wc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klsum(document, summary, L):\n",
    "    doc_sent = sent_tokenize(document)\n",
    "    doc_wc = wc(document)\n",
    "    doc_pd = pd(doc_wc)\n",
    "    px = [p for p in doc_pd.values()]\n",
    "    \n",
    "    while len(sent_tokenize(summary)) < L:\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        _min, _min_id = 999, -1\n",
    "        for idx, sent in enumerate(sentences):\n",
    "            new_sum = summary + sent\n",
    "            new_pd = pd(wc(new_sum))\n",
    "            qx = [new_pd[k] if k in new_pd else 0.001 for k in doc_pd.keys()]\n",
    "            kl = kl_div(px, qx)\n",
    "            if kl < _min:\n",
    "                _min, _min_id = kl, idx\n",
    "        \n",
    "        summary += \"\\n\" + sentences[_min_id]\n",
    "        document = \" \".join(sentences[:_min_id] + sentences[_min_id+1:])\n",
    "    \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_summaries = {}\n",
    "for d in data:\n",
    "    kl_summaries[d['doc_id']] = klsum(d['doc_text'], '', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDASum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldasum(data, summary, topics, L):\n",
    "    doc = data['doc_text']\n",
    "    doc_topics = [int(t.strip()) for t in data['doc_topics'].split(',')]\n",
    "    doc_topics_pd = [float(t.strip()) for t in data['doc_topics_pd'].split(',')]\n",
    "    \n",
    "    while len(sent_tokenize(summary)) < L:\n",
    "        sentences = sent_tokenize(doc)\n",
    "        score = [0] * len(sentences)\n",
    "        for idx, topic in enumerate(topics):\n",
    "            if topic['topic_id'] in doc_topics:\n",
    "                for sidx, sent in enumerate(sentences):\n",
    "                    for word in word_tokenize(sent):\n",
    "                        if word in topic['top_words']:\n",
    "                            score[sidx] += topic['word_prob'][topic['top_words'].index(word)]\n",
    "        \n",
    "        best = np.argmax(score)\n",
    "        summary += ' \\n ' + sentences[best]\n",
    "        doc = \" \".join(sentences[:best] + sentences[best+1:])\n",
    "        \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 301/301 [00:13<00:00, 22.96it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_summaries = {}\n",
    "for d in tqdm(data):\n",
    "    lda_summaries[d['doc_id']] = ldasum(d, '', topics, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 301/301 [00:19<00:00, 15.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(data):\n",
    "    _id = d['doc_id']\n",
    "    d['kl_summary'] = kl_summaries[_id]\n",
    "    d['lda_summary'] = lda_summaries[_id]\n",
    "    \n",
    "    es.index(index='ducsummary', doc_type='doc', body=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TopicSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=-1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=666, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2, max_features=1000)\n",
    "features = vectorizer.fit_transform(duc_data)\n",
    "model = LatentDirichletAllocation(n_components=10, random_state=666, learning_method='online', n_jobs=-1)\n",
    "model.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topicsum(document, summary, L):\n",
    "    px = model.transform(vectorizer.transform([document]))[0]\n",
    "    \n",
    "    while len(sent_tokenize(summary)) < L:\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        _min, _min_id = 999, -1\n",
    "        for idx, sent in enumerate(sentences):\n",
    "            new_sum = summary + sent\n",
    "            qx = model.transform(vectorizer.transform([new_sum]))[0]\n",
    "            kl = kl_div(px, qx)\n",
    "            if kl < _min:\n",
    "                _min, _min_id = kl, idx\n",
    "        \n",
    "        summary += \"\\n\" + sentences[_min_id]\n",
    "        document = \" \".join(sentences[:_min_id] + sentences[_min_id+1:])\n",
    "    \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The group contends that including the estimated 2 million or\n",
      "more illegal aliens in the national head count, which is used to\n",
      "distribute seats in the House of Representatives, will cause unfair\n",
      "shifts of seats from one state to another.\n",
      "Some 40 members of the House joined the Federation for American\n",
      "Immigration Reform in announcing that the suit would be filed\n",
      "Thursday in U.S. District Court in Pittsburgh, spokesmen said at a\n",
      "news conference here.\n"
     ]
    }
   ],
   "source": [
    "print(topicsum(docs['AP880217-0175'], '', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
