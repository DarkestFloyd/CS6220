{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import itertools, operator\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def parse_file(path):\n",
    "    data = []\n",
    "    with open(path) as infile:\n",
    "        lines = infile.readlines()\n",
    "        for line in lines:\n",
    "            splits = line.split(',')\n",
    "            data.append([int(splits[0][1:]), splits[1].strip()[1:-1], ' '.join(splits[2:]).strip()[1:-2]])\n",
    "    return data\n",
    "\n",
    "pb2_data = parse_file('../data/exam/pb2_data.txt')\n",
    "pb2_data_full = parse_file('../data/exam/pb2_data_full.txt')\n",
    "\n",
    "pb2_topics = np.unique([i[1] for i in pb2_data])\n",
    "topic_map = {v: k for k, v in enumerate(pb2_topics)}\n",
    "\n",
    "for d in pb2_data:\n",
    "    d.append(topic_map[d[1]])\n",
    "\n",
    "ytrue = np.array([i[3] for i in pb2_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def group_by_label(l):\n",
    "    it = itertools.groupby(l, operator.itemgetter(1))\n",
    "    counts = []\n",
    "    for key, subiter in it:\n",
    "        counts.append(sum(item[0] for item in subiter))\n",
    "    return counts\n",
    "\n",
    "def compute_homogeneity(preds, labels):\n",
    "    cluster_label_counts = []\n",
    "    for pred in preds.transpose():\n",
    "        cluster_label_counts.append(group_by_label([(p,label) for p,label in zip(pred,labels)]))\n",
    "    \n",
    "    entropys = []\n",
    "    for cluster_label_count in cluster_label_counts:\n",
    "        entropys.append(scipy.stats.entropy(cluster_label_count))\n",
    "         \n",
    "    return np.mean(entropys)\n",
    "\n",
    "def compute_completeness(preds, labels, num_clusters, num_labels):\n",
    "    label_cluster_counts = {label:np.zeros(num_clusters) for label in range(num_labels)}\n",
    "    \n",
    "    for pred, label in zip(preds, labels):\n",
    "        label_cluster_counts[label] = np.sum([label_cluster_counts[label], pred], axis=0)\n",
    "    \n",
    "    entropys = []\n",
    "    for label_cluster_count in label_cluster_counts.values():\n",
    "        entropys.append(scipy.stats.entropy(label_cluster_count))\n",
    "          \n",
    "    return np.mean(entropys)\n",
    "\n",
    "\n",
    "def v_measure(preds, labels, num_clusters, num_labels):\n",
    "    if len(labels) == 0:\n",
    "        return 1.0, 1.0, 1.0\n",
    "      \n",
    "    homogeneity = compute_homogeneity(preds, labels)\n",
    "    completeness = compute_completeness(preds, labels, num_clusters, num_labels)\n",
    "    \n",
    "    if homogeneity==0.0 and completeness==0.0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    v_measure_score = (2.0 * homogeneity * completeness /\n",
    "                   (homogeneity + completeness))\n",
    "      \n",
    "    return homogeneity, completeness, v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, \n",
    "                        min_df=2, max_features=1000)\n",
    "features = vectorizer.fit_transform([d[-1] for d in pb2_data_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 9.82 s, total: 1min 50s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LDA(n_components=50, random_state=666, learning_method='online', n_jobs=-1)\n",
    "lda.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sub = lda.transform(features[[i[0] for i in pb2_data]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=20, covariance_type='full').fit(feature_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = gm.predict_proba(feature_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.4229688422600804, 2.2604688931316064, 2.338899759707366)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_measure(yhat, ytrue, 20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lda for t=5\n",
      "gm for k=10\n",
      "\n",
      "gm for k=20\n",
      "\n",
      "fitting lda for t=10\n",
      "gm for k=10\n",
      "\n",
      "gm for k=20\n",
      "\n",
      "fitting lda for t=20\n",
      "gm for k=10\n",
      "\n",
      "gm for k=20\n",
      "\n",
      "fitting lda for t=50\n",
      "gm for k=20\n",
      "\n",
      "CPU times: user 2min 26s, sys: 12.1 s, total: 2min 38s\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "%%time\n",
    "_log = {}\n",
    "for t in [5, 10, 20, 50]:\n",
    "    if t not in _log: _log[t] = {}\n",
    "    print('fitting lda for t=%d' % t)\n",
    "    lda = LDA(n_components=t, random_state=666, learning_method='online', n_jobs=-1)\n",
    "    lda.fit(features)\n",
    "    feature_sub = lda.transform(features[[i[0] for i in pb2_data]])\n",
    "    for k in [10, 20]:\n",
    "        if k == 10 and t == 50:\n",
    "            continue\n",
    "        if k not in _log[t]: _log[t][k] = {}\n",
    "        print('gm for k=%d' % k)\n",
    "        gm = GaussianMixture(n_components=k, covariance_type='full').fit(feature_sub)\n",
    "        ypred = gm.predict_proba(feature_sub)\n",
    "        _log[t][k]['preds'] = ypred\n",
    "        _log[t][k]['score'] = v_measure(ypred, ytrue, k, 20)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=10, t=5; scores => (2.2508772199057216, 1.452443332602814, 1.7655893213700347)\n",
      "For k=20, t=5; scores => (2.149050214457521, 2.021878199290378, 2.083525463285308)\n",
      "For k=10, t=10; scores => (2.3490649477850067, 1.6331981971545126, 1.926788109217026)\n",
      "For k=20, t=10; scores => (2.2120924175451955, 2.0636307314306075, 2.1352841306876327)\n",
      "For k=10, t=20; scores => (2.469589982940709, 1.6879094517421909, 2.0052651068857656)\n",
      "For k=20, t=20; scores => (2.2897352572189655, 2.16601332968742, 2.2261566117826983)\n",
      "For k=20, t=50; scores => (2.3345403962827347, 2.255390308879039, 2.2942829091260255)\n"
     ]
    }
   ],
   "source": [
    "for t in _log:\n",
    "    for k in _log[t]:\n",
    "        print(\"For k=%d, t=%d; scores =>\" % (k, t), _log[t][k]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For k=10 and t=5, we get the best v_measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using k=10 and t=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.35 s, sys: 370 ms, total: 6.72 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LDA(n_components=5, random_state=666, learning_method='online', n_jobs=-1)\n",
    "lda.fit(features)\n",
    "feature_sub = lda.transform(features[[i[0] for i in pb2_data]])\n",
    "gm = GaussianMixture(n_components=10, covariance_type='full').fit(feature_sub)\n",
    "yhat = gm.predict_proba(feature_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest cluster: 6 with total 1200.929314 memeberships\n"
     ]
    }
   ],
   "source": [
    "_biggest = np.argmax(np.sum(yhat, axis=0))\n",
    "print(\"Biggest cluster: %d with total %f memeberships\" % (_biggest, np.sum(yhat, axis=0)[_biggest]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biggest soft cluster is cluster 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ax max g9v b8f stephanopoulos\n",
      "Topic #1: like know just don use\n",
      "Topic #2: 10 20 12 15 00\n",
      "Topic #3: people don god think just\n",
      "Topic #4: edu file space use information\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, vectorizer.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
